import numpy as np

# Навчальний набір даних
X_train = np.array([
    [0.0228, 0.0601, 0.2925, 0.2111, 0.3061, 0.1075],
    [0.0409, 0.2746, 0.0806, 0.3014, 0.1867, 0.1159],
    [0.0816, 0.2465, 0.0968, 0.3149, 0.1635, 0.0967],
    [0.0467, 0.2742, 0.0903, 0.3317, 0.1615, 0.0955],
    [0.0241, 0.2508, 0.115, 0.321, 0.1822, 0.1068],
    [0.0558, 0.1306, 0.2822, 0.2324, 0.1475, 0.1516],
    [0.0591, 0.1454, 0.4453, 0.1453, 0.1037, 0.1011],
    [0.0659, 0.1175, 0.2734, 0.2632, 0.119, 0.161],
    [0.034, 0.1444, 0.3095, 0.2068, 0.1562, 0.1491],
    [0.0825, 0.148, 0.378, 0.1472, 0.1522, 0.0921]

])
y_train = np.array([1, 1, 1, 1, 1, -1, -1, -1, -1, -1])  # Мітки для навчального набору

# Тестовий набір
X_test = np.array([0.0567, 0.1303, 0.3547, 0.1901, 0.1584, 0.1098])

# Ініціалізація ваг та зсуву
weights = np.random.rand(6)  # Випадкові початкові ваги для 4 ознак
bias = 0.0

# Параметри
learning_rate = 0.1
epochs = 10  # Кількість ітерацій

# Функція активації (порогова функція)
def activation_function(z):
    return 1 if z >= 0 else -1

# Навчання перцептрона
for epoch in range(epochs):
    errors = 0  # Лічильник помилок
    for i in range(len(X_train)):
        z = np.dot(X_train[i], weights) + bias
        y_pred = activation_function(z)

        if y_pred != y_train[i]:
            weights += learning_rate * (y_train[i] - y_pred) * X_train[i]
            bias += learning_rate * (y_train[i] - y_pred)
            errors += 1  # Підрахунок помилок

    print(f"Епоха {epoch + 1}, помилок: {errors}")

    if errors == 0:
        print("Навчання завершено достроково.")
        break

# Підсумкові ваги та зсув
print("Підсумкові ваги:", weights)
print("Підсумковий зсув:", bias)

# Тестування на нових даних
z_test = np.dot(X_test, weights) + bias
y_test_pred = activation_function(z_test)

print("Результат для тестового набору:", y_test_pred)
